{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : natural language processing : sentiment analysis <br>\n",
    "Execution time : 15 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json as j\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.util import mark_negation\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will perform sentiment analysis using the dataset of amazon reviews.<br> \n",
    "The sentiment classifications are the different scores a reviewer can give ranging from 0 to 5.<br> \n",
    "Since it is generally agreed that being pretty is important, we are going to use the beauty dataset to find the best products. <br> \n",
    "This can be changed easily by downloading another dataset from : http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define settings\n",
    "data = pd.read_json('/home/jasper/Documents/master/nlp/amazon/data/Beauty_5.json', lines=True)\n",
    "toStem = False #use stemmer or not\n",
    "toLem = True  #use lemmatizer or not\n",
    "words = set(stopwords.words('english')) #remove stopwords or [] to do nothing\n",
    "discretizeLabels = True  #discretize labels to 3 or not\n",
    "K_best_features = 10000  # amaount of features selected from n_grams\n",
    "max_review_amount = 20000 # max amount of reviews per classification for balancing\n",
    "N_grams = [1,2,3,4,5]  # n_grams to test\n",
    "if discretizeLabels:\n",
    "    numberOfLabels = 3\n",
    "else: \n",
    "    numberOfLabels = 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will write the preprocessing that happens to the text. <br> \n",
    "(optionally) Use stemmer : this normalises different variations of a word such as product and products.<br> \n",
    "(optionally) Use lemmatizer : this normalises different variations of a word such as better and good.<br> \n",
    "(optionally) Removing stopwords, these are all words recognised to contain very little information.<br> \n",
    "Mark negations in sentences, this processed sentences such as \"I don't like it.\" to \"I don't NEG_like NEG_it.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmanizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmerize = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def convertToTokens(text):\n",
    "    text =  \" \".join(nltk.sentiment.util.mark_negation(t) for t in nltk.sent_tokenize(text))\n",
    "    text = text.replace(',', ' ')\n",
    "    text = text.replace('/', ' ')\n",
    "    text = text.replace('(', ' ')\n",
    "    text = text.replace(')', ' ')\n",
    "    text = text.replace('.', ' ')\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    #tokens = [nltk.sentiment.util.mark_negation(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in words]\n",
    "    if toLem:\n",
    "        tokens = [lemmanizer.lemmatize(word) for word in tokens]\n",
    "    if toStem:\n",
    "        tokens = [stemmerize.stem(word) for word in tokens]\n",
    "    return \" \".join(str(x) for x in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optionally) We discretize the classifications as follows:<br> \n",
    "a score of 1 or 2 -> negative (1)<br> \n",
    "a score of 3      -> neutral  (2)<br> \n",
    "a score of 4 or 5 -> positive (3)<br> \n",
    "\n",
    "We also balance the dataset to contain <=max_review_amount for each label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20000. 20000. 20000.]\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "overall = []\n",
    "review_to_data_index = []\n",
    "amounts = np.zeros(numberOfLabels)\n",
    "for i, score in enumerate(data[\"overall\"]):\n",
    "    if score == 3 and discretizeLabels :\n",
    "        score=2\n",
    "    elif score <= 2 and discretizeLabels:\n",
    "        score=1\n",
    "    elif score >= 4 and discretizeLabels:\n",
    "        score=3\n",
    "    review = data[\"reviewText\"][i]\n",
    "    if amounts[score-1] < max_review_amount:\n",
    "        amounts[score-1] += 1\n",
    "        reviews.append(review)\n",
    "        overall.append(score)\n",
    "print(amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spilt the training set to 80% training and 20% test set and tokenize every review.\n",
    "The original reviews are also kept in memory to later access when evaluating misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_, X_test_, y_train, y_test = train_test_split(reviews, overall, test_size=0.2)\n",
    "X_train = [convertToTokens(review) for review in X_train_]\n",
    "X_test = [convertToTokens(review) for review in X_test_]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define code to evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "#Prints out the confusion matrix aswell as precision recall and F-score for each class\n",
    "def printResult(y_test,X_test, model):\n",
    "    print(\"accuracy score: \" + str(model.score(X_test, y_test)))\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n",
    "    print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "#prints the top features for each classification chosen by CHI2 correlation meassure    \n",
    "def printTopResults(model):\n",
    "    vectorizer = model.named_steps['vect']\n",
    "    chi = model.named_steps['chi']\n",
    "    clf = model.named_steps['clf']\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    feature_names = [feature_names[i] for i in chi.get_support(indices=True)]\n",
    "    feature_names = np.asarray(feature_names)\n",
    "    if discretizeLabels:\n",
    "        target_names = ['1', '2', '3']\n",
    "    else:\n",
    "        target_names = ['1', '2', '3','4','5']\n",
    "    print(\"top 10 keywords per class:\")\n",
    "    for i, label in enumerate(target_names):\n",
    "        top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (label, \" | \".join(feature_names[top10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction <br> \n",
    "After preprocessing the data we define the feature extraction, for this we will use TfidfVectorizer. <br>\n",
    "This first computes the n_grams and converts the input text to count vectors. <br>\n",
    "Then it performs TfidfTransformer which vectorizes the count vector, by setting sublinear_tf True smooth out the results. <br>\n",
    "\n",
    "\n",
    "\n",
    "For classification we will be using the pipeline tool from sklearn for the following algorithms: <br> \n",
    "1) Suport vector machines <br> \n",
    "2) Naive bayes classifier <br> \n",
    "3) Random forest <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#using TfidfVectorizer which is equivalent to CountVectorizer followed by TfidfTransformer.\n",
    "#we choose the best features from n_grams from 1 till n\n",
    "vectorizer = lambda n_gram : TfidfVectorizer(ngram_range=(n_gram, n_gram), sublinear_tf=True)\n",
    "#selecting n best features using the chi2 test\n",
    "chi = SelectKBest(chi2, k=K_best_features)\n",
    "\n",
    "SVC = LinearSVC(C=1.0, penalty='l1', max_iter=30000, dual=False)\n",
    "NB = MultinomialNB()\n",
    "RF = RandomForestClassifier()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training SVC with N_gram features chosen from :1\n",
      "\n",
      "accuracy score: 0.6795\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.67      0.68      4072\n",
      "           2       0.61      0.57      0.59      3999\n",
      "           3       0.74      0.79      0.77      3929\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     12000\n",
      "   macro avg       0.68      0.68      0.68     12000\n",
      "weighted avg       0.68      0.68      0.68     12000\n",
      "\n",
      "[[2748  959  365]\n",
      " [ 987 2292  720]\n",
      " [ 292  523 3114]]\n",
      "top 10 keywords per class:\n",
      "1: confirmed | scunci | gasoline | inexcusable | zo | hunk | trashed | rouge | jerome | accumulated\n",
      "2: indifferent | rebuilding | uncontrollable | swapped | yanked | mentally | ainhoa | afar | summary | wiff\n",
      "3: shriek | froze | ordinatry | 26th | rebecca | love | hooked | cabochard | hangnail | ventilation\n",
      "\n",
      " Training SVC with N_gram features chosen from :2\n",
      "\n",
      "accuracy score: 0.6219166666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.65      0.63      4072\n",
      "           2       0.55      0.48      0.51      3999\n",
      "           3       0.69      0.74      0.72      3929\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     12000\n",
      "   macro avg       0.62      0.62      0.62     12000\n",
      "weighted avg       0.62      0.62      0.62     12000\n",
      "\n",
      "[[2628 1009  435]\n",
      " [1232 1919  848]\n",
      " [ 459  554 2916]]\n",
      "top 10 keywords per class:\n",
      "1: wasted money | full refund | threw away | hard earned | huge disappointment | zero star | money back | waste money | two star | one worst\n",
      "2: lathered nicely | overall bad | overall re | going spend | overly impressed | plus side | like short | make decent | front mirror | three star\n",
      "3: remember first | get compliment | pleasantly surprised | ca stop | glad found | love stuff | stuff amazing | the rebecca | say enough | highly recommend\n",
      "\n",
      " Training SVC with N_gram features chosen from :3\n",
      "\n",
      "accuracy score: 0.46116666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.25      0.35      4072\n",
      "           2       0.38      0.76      0.50      3999\n",
      "           3       0.64      0.38      0.48      3929\n",
      "\n",
      "   micro avg       0.46      0.46      0.46     12000\n",
      "   macro avg       0.54      0.46      0.44     12000\n",
      "weighted avg       0.54      0.46      0.44     12000\n",
      "\n",
      "[[1003 2747  322]\n",
      " [ 433 3020  546]\n",
      " [ 177 2241 1511]]\n",
      "top 10 keywords per class:\n",
      "1: get money back | waste money time | give two star | complete waste money | hard earned money | total waste money | would never buy | giving two star | waste time money | gave two star\n",
      "2: ll probably stick | ll keep trying | mixed feeling product | giving three star | probably would purchase | banana boat sport | product three star | would go way | give three star | gave three star\n",
      "3: never go back | amazon best price | work like charm | always come back | product highly recommend | dead sea mud | highly recommend product | worth every penny | took one star | the rebecca review\n",
      "\n",
      " Training SVC with N_gram features chosen from :4\n",
      "\n",
      "accuracy score: 0.36075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.04      0.07      4072\n",
      "           2       0.34      0.96      0.50      3999\n",
      "           3       0.66      0.09      0.16      3929\n",
      "\n",
      "   micro avg       0.36      0.36      0.36     12000\n",
      "   macro avg       0.55      0.36      0.24     12000\n",
      "weighted avg       0.55      0.36      0.24     12000\n",
      "\n",
      "[[ 152 3857   63]\n",
      " [  56 3821  122]\n",
      " [  26 3547  356]]\n",
      "top 10 keywords per class:\n",
      "1: made hair feel like | sun without reddening 15 | thick coily aa hair | could give star would | many positive review product | use long enough see | would never recommend anyone | two star instead one | really wanted like product | could give zero star\n",
      "2: ve using shampoo conditioner | 90 second wrinkle reducer | leave face feeling clean | nexxus youth renewal rejuvenating | something else next time | make up remover pad | clear scalp hair beauty | gave product three star | ve using two week | reason gave three star\n",
      "3: use tea tree oil | little go along way | hope never stop making | love love love stuff | like tea tree oil | great product great price | always come back one | little go long way | say enough good thing | dead sea mud soap\n",
      "\n",
      " Training SVC with N_gram features chosen from :5\n",
      "\n",
      "accuracy score: 0.32966666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.01      0.01      4072\n",
      "           2       0.60      0.00      0.01      3999\n",
      "           3       0.33      1.00      0.49      3929\n",
      "\n",
      "   micro avg       0.33      0.33      0.33     12000\n",
      "   macro avg       0.59      0.34      0.17     12000\n",
      "weighted avg       0.59      0.33      0.17     12000\n",
      "\n",
      "[[  21    3 4048]\n",
      " [   3   12 3984]\n",
      " [   1    5 3923]]\n",
      "top 10 keywords per class:\n",
      "1: wish could get money back | caused break outs first use | see surfer put nose protection | sheer blonde colour renew tone | heard many great thing product | could give zero star would | organ system toxicity non reproductive | made hair feel like straw | gave two star instead one | oxide physical sunscreen rather chemical\n",
      "2: beauty therapy frizz control nourishing | try something different next time | clear scalp hair beauty strong | platinum strength deep conditioning treatment | scalp hair beauty strong length | nexxus youth renewal rejuvenating elixir | try something else next time | professional moroccan infusion shine conditioner | scalp hair beauty ultra shea | eye make up remover pad\n",
      "3: love way make skin feel | little bit go long way | little go long way use | queen helene mint julep masque | little go long way last | small amount go long way | little go long way bottle | dove sensitive skin body wash | adovia dead sea mud soap | ca say enough good thing\n"
     ]
    }
   ],
   "source": [
    "for n in N_grams:\n",
    "    print (\"\\n Training SVC with N_gram features chosen from :\" + str(n)+ \"\\n\")\n",
    "    vect = vectorizer(n)\n",
    "    vect = vect.fit(X_train)\n",
    "    pipeline = Pipeline([('vect', vect),\n",
    "                         ('chi',  chi),\n",
    "                         ('clf', SVC)])\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    printResult(y_test,X_test, model)\n",
    "    printTopResults(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Naive Bayes with N_gram features chosen from :1\n",
      "\n",
      "accuracy score: 0.6650833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.62      0.65      4072\n",
      "           2       0.58      0.63      0.60      3999\n",
      "           3       0.74      0.75      0.74      3929\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     12000\n",
      "   macro avg       0.67      0.67      0.67     12000\n",
      "weighted avg       0.67      0.67      0.67     12000\n",
      "\n",
      "[[2511 1156  405]\n",
      " [ 833 2519  647]\n",
      " [ 295  683 2951]]\n",
      "top 10 keywords per class:\n",
      "1: even | really | used | get | one | use | skin | would | like | product\n",
      "2: one | smell | color | really | skin | good | would | use | like | product\n",
      "3: really | one | good | smell | like | use | great | skin | product | love\n",
      "\n",
      " Training Naive Bayes with N_gram features chosen from :2\n",
      "\n",
      "accuracy score: 0.62875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.57      0.62      4072\n",
      "           2       0.56      0.53      0.55      3999\n",
      "           3       0.64      0.79      0.71      3929\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     12000\n",
      "   macro avg       0.63      0.63      0.63     12000\n",
      "weighted avg       0.63      0.63      0.62     12000\n",
      "\n",
      "[[2313 1079  680]\n",
      " [ 838 2134 1027]\n",
      " [ 244  587 3098]]\n",
      "top 10 keywords per class:\n",
      "1: work well | much better | thought would | made hair | sensitive skin | feel like | smell like | look like | would recommend | waste money\n",
      "2: last long | look like | sensitive skin | would recommend | ve used | dry skin | would buy | thought would | feel like | work well\n",
      "3: would recommend | last long | sensitive skin | love product | dry skin | long time | work great | great product | highly recommend | work well\n",
      "\n",
      " Training Naive Bayes with N_gram features chosen from :3\n",
      "\n",
      "accuracy score: 0.4364166666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.25      0.36      4072\n",
      "           2       0.52      0.18      0.26      3999\n",
      "           3       0.39      0.89      0.54      3929\n",
      "\n",
      "   micro avg       0.44      0.44      0.44     12000\n",
      "   macro avg       0.51      0.44      0.39     12000\n",
      "weighted avg       0.52      0.44      0.39     12000\n",
      "\n",
      "[[1029  388 2655]\n",
      " [ 430  706 2863]\n",
      " [ 173  254 3502]]\n",
      "top 10 keywords per class:\n",
      "1: recommend product anyone | product made hair | acne prone skin | work much better | total waste money | waste time money | really wanted like | would recommend anyone | made hair feel | would recommend product\n",
      "2: nail polish remover | work pretty well | little go long | acne prone skin | last long time | make skin feel | product work well | gave three star | go long way | get job done\n",
      "3: highly recommend product | work really well | would definitely recommend | tea tree oil | use every day | make skin feel | love love love | little go long | last long time | go long way\n",
      "\n",
      " Training Naive Bayes with N_gram features chosen from :4\n",
      "\n",
      "accuracy score: 0.34425\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.04      0.07      4072\n",
      "           2       0.59      0.02      0.04      3999\n",
      "           3       0.33      0.99      0.50      3929\n",
      "\n",
      "   micro avg       0.34      0.34      0.34     12000\n",
      "   macro avg       0.53      0.35      0.21     12000\n",
      "weighted avg       0.53      0.34      0.20     12000\n",
      "\n",
      "[[ 160   40 3872]\n",
      " [  56   88 3855]\n",
      " [  25   21 3883]]\n",
      "top 10 keywords per class:\n",
      "1: proactiv product overpriced undersized | waste money would recommend | waste hard earned money | made hair feel like | thought would give try | hair feel like straw | look nothing like picture | really wanted like product | little go long way | would recommend product anyone\n",
      "2: something else next time | thought would give try | oily acne prone skin | scalp hair beauty therapy | wash hair every day | little bit go long | reason gave three star | bit go long way | clear scalp hair beauty | little go long way\n",
      "3: make skin feel soft | wash hair every day | great product great price | make hair soft shiny | leaf skin feeling soft | bottle last long time | oily acne prone skin | little bit go long | bit go long way | little go long way\n",
      "\n",
      " Training Naive Bayes with N_gram features chosen from :5\n",
      "\n",
      "accuracy score: 0.33008333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.01      0.01      4072\n",
      "           2       0.61      0.00      0.01      3999\n",
      "           3       0.33      1.00      0.49      3929\n",
      "\n",
      "   micro avg       0.33      0.33      0.33     12000\n",
      "   macro avg       0.58      0.34      0.17     12000\n",
      "weighted avg       0.58      0.33      0.17     12000\n",
      "\n",
      "[[  25    4 4043]\n",
      " [   4   14 3981]\n",
      " [   2    5 3922]]\n",
      "top 10 keywords per class:\n",
      "1: dried scalp smelled great though | nice price kind cheap looking | anything saying would work others | used waist shrink saw difference | uneven hard cut uneven good | horrible cakey concealer waste money | expecting know alter brush perfect | return christmas morning light work | good price fragrance stay longer | made hair feel like straw\n",
      "2: good quality like smell recomend | work ok break easily wear | really like color selection quality | brush mucher smaller expected cute | clear scalp hair beauty therapy | seemed mishandled scratch plate stamp | plate seemed mishandled scratch plate | mishandled scratch plate stamp fine | try something else next time | little bit go long way\n",
      "3: product little go long way | work great damaged colored hair | great damaged colored hair profitable | damaged colored hair profitable order | work well last long time | love way make skin feel | use base coat top coat | love smell last long time | little go long way use | little bit go long way\n"
     ]
    }
   ],
   "source": [
    "for n in N_grams:\n",
    "    print (\"\\n Training Naive Bayes with N_gram features chosen from :\" + str(n)+ \"\\n\")\n",
    "    vect = vectorizer(n)\n",
    "    vect = vect.fit(X_train)\n",
    "    pipeline = Pipeline([('vect', vect),\n",
    "                         ('chi',  chi),\n",
    "                         ('clf', NB)])\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    printResult(y_test,X_test, model)\n",
    "    printTopResults(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Random Forest with N_gram features chosen from : 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jasper/Downloads/enter/envs/DL_36/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.5616666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.59      0.57      4072\n",
      "           2       0.49      0.47      0.48      3999\n",
      "           3       0.64      0.62      0.63      3929\n",
      "\n",
      "   micro avg       0.56      0.56      0.56     12000\n",
      "   macro avg       0.56      0.56      0.56     12000\n",
      "weighted avg       0.56      0.56      0.56     12000\n",
      "\n",
      "[[2420 1087  565]\n",
      " [1292 1899  808]\n",
      " [ 642  866 2421]]\n",
      "\n",
      " Training Random Forest with N_gram features chosen from : 2\n",
      "\n",
      "accuracy score: 0.53875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.62      0.57      4072\n",
      "           2       0.48      0.40      0.44      3999\n",
      "           3       0.61      0.60      0.60      3929\n",
      "\n",
      "   micro avg       0.54      0.54      0.54     12000\n",
      "   macro avg       0.54      0.54      0.54     12000\n",
      "weighted avg       0.54      0.54      0.54     12000\n",
      "\n",
      "[[2518  961  593]\n",
      " [1493 1603  903]\n",
      " [ 785  800 2344]]\n",
      "\n",
      " Training Random Forest with N_gram features chosen from : 3\n",
      "\n",
      "accuracy score: 0.4415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.24      0.33      4072\n",
      "           2       0.37      0.74      0.49      3999\n",
      "           3       0.61      0.35      0.44      3929\n",
      "\n",
      "   micro avg       0.44      0.44      0.44     12000\n",
      "   macro avg       0.51      0.44      0.42     12000\n",
      "weighted avg       0.51      0.44      0.42     12000\n",
      "\n",
      "[[ 978 2758  336]\n",
      " [ 510 2962  527]\n",
      " [ 295 2276 1358]]\n",
      "\n",
      " Training Random Forest with N_gram features chosen from : 4\n",
      "\n",
      "accuracy score: 0.3595\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.04      0.07      4072\n",
      "           2       0.34      0.95      0.50      3999\n",
      "           3       0.66      0.09      0.15      3929\n",
      "\n",
      "   micro avg       0.36      0.36      0.36     12000\n",
      "   macro avg       0.53      0.36      0.24     12000\n",
      "weighted avg       0.53      0.36      0.24     12000\n",
      "\n",
      "[[ 158 3853   61]\n",
      " [  67 3816  116]\n",
      " [  38 3551  340]]\n",
      "\n",
      " Training Random Forest with N_gram features chosen from : 5\n",
      "\n",
      "accuracy score: 0.33925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.85      0.01      0.01      4072\n",
      "           2       0.34      1.00      0.50      3999\n",
      "           3       0.79      0.02      0.03      3929\n",
      "\n",
      "   micro avg       0.34      0.34      0.34     12000\n",
      "   macro avg       0.66      0.34      0.18     12000\n",
      "weighted avg       0.66      0.34      0.18     12000\n",
      "\n",
      "[[  23 4042    7]\n",
      " [   3 3987    9]\n",
      " [   1 3867   61]]\n"
     ]
    }
   ],
   "source": [
    "for n in N_grams:\n",
    "    print (\"\\n Training Random Forest with N_gram features chosen from : \" + str(n)+ \"\\n\")\n",
    "    vect = vectorizer(n)\n",
    "    vect = vect.fit(X_train)\n",
    "    pipeline = Pipeline([('vect', vect),\n",
    "                         ('chi',  chi),\n",
    "                         ('clf', RF)])\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    printResult(y_test,X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sentiment lexigon as features <br>\n",
    "In this small sample we will be using the vader sentiment lexicon as features for training. We also adopt 2 self chosen features being the length of the text, and number of capital letters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 6)\n",
      "(12000, 6)\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "\n",
    "def get_lexicon_features(doc):\n",
    "    vals = analyser.polarity_scores(doc)\n",
    "    res = [vals[val] for val in vals]\n",
    "    Num_caps = sum(1 for c in doc if c.isupper())\n",
    "    res.append(Num_caps) #adding number of letters in caps\n",
    "    res.append(len(doc)) # adding length of review\n",
    "    return np.asarray(res)\n",
    "    \n",
    "X_train_lexicon = np.asarray([get_lexicon_features(doc) for doc in X_train_])\n",
    "X_test_lexicon = np.asarray([get_lexicon_features(doc) for doc  in X_test_])\n",
    "print(X_train_lexicon.shape)\n",
    "print(X_test_lexicon.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.51225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.58      0.57      4072\n",
      "           2       0.40      0.30      0.34      3999\n",
      "           3       0.54      0.66      0.59      3929\n",
      "\n",
      "   micro avg       0.51      0.51      0.51     12000\n",
      "   macro avg       0.50      0.51      0.50     12000\n",
      "weighted avg       0.50      0.51      0.50     12000\n",
      "\n",
      "[[2368  936  768]\n",
      " [1331 1185 1483]\n",
      " [ 497  838 2594]]\n"
     ]
    }
   ],
   "source": [
    "SVC = LinearSVC(C=1.0, penalty='l1', max_iter=30000, dual=False)\n",
    "RF = RandomForestClassifier()\n",
    "\n",
    "SVC.fit(X_train_lexicon,y_train)\n",
    "printResult(y_test,X_test_lexicon, SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.47583333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.55      0.53      4072\n",
      "           2       0.37      0.35      0.36      3999\n",
      "           3       0.54      0.53      0.53      3929\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     12000\n",
      "   macro avg       0.47      0.48      0.47     12000\n",
      "weighted avg       0.47      0.48      0.47     12000\n",
      "\n",
      "[[2243 1224  605]\n",
      " [1471 1399 1129]\n",
      " [ 740 1121 2068]]\n"
     ]
    }
   ],
   "source": [
    "RF.fit(X_train_lexicon,y_train)\n",
    "printResult(y_test,X_test_lexicon, RF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discussing results <br> \n",
    "Generally we can see that with every algorithm the unigram features reached best performance. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print 10 misclassifications, I chose to do this for the SVM with 1_gram input as this generally performed quite decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = vectorizer(1)\n",
    "vect = vect.fit(X_train)\n",
    "pipeline = Pipeline([('vect', vect),\n",
    "                    ('chi',  chi),\n",
    "                    ('clf', SVC)])\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "missclassifications = []\n",
    "\n",
    "ypred= np.zeros([len(X_test)])\n",
    "for i, f in enumerate(X_test):\n",
    "    ypred[i] = int(model.predict([f]))\n",
    "    \n",
    "    \n",
    "for i in range(0,len(y_test)):\n",
    "    if (int(ypred[i]) != y_test[i]):\n",
    "        missclassifications.append(i) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review : I'm not a big fan of this product. I love the \"smoky eyes\" look so I thought this product would be exactly what I need. I could barely tell I had it on, which was not the look I was going for. It also smeared a little too much without giving me the look that I wanted. With so many eyeliners on the market, including others from Avon, I wouldn't put this product on my \"must get\" list. However, to be fair, I only tried it in the black so if you are using another color, you probably will want a softer expression - meaning you notice the color without REALLY NOTICING the color. In that case it may work well for you.\n",
      "classified as : 2.0\n",
      "true label was : 1\n",
      "review : I have given up on this eye liner.  Every time I've worn it, I've ended up with smears under my eyes.  It's a bummer because it goes on so easily.  I may try the waterproof version to see if that's any better.\n",
      "classified as : 1.0\n",
      "true label was : 2\n",
      "review : If you are like me then you are looking for truly natural products for your body (and to put in your body, in the case of food and drink). Suave is known for being a low cost option and if they have a real natural product then you've got the ingredients for a top notch, budget natural product. But the question is: is this really a \"natural\" product? The answer is found in how you define \"is\" and \"natural\". (Thanks, Bill Clinton ;)There are no parabens (preservatives) in the ingredient list, which is great news as they've been linked to all kinds of nasty stuff including cancer. But there are sulfates (foaming agents), in the form of Ammonium Laureth Sulfate, which also has been linked to cancer. So is this really a natural product? Or this a marketing gimmick? (Notice that the product NAME not the product DESCRIPTION is \"natural.\") There are also other chemicals in the ingredient list as well.Since all these ingredients exist in the physical, or natural, world I guess you could call a shampoo or conditioner that has only physical things in it as \"natural.\" But if you are looking for non-man-made-chemical \"natural\" then this product does NOT meet that definition. So it isn't really what it's saying it is.Why three stars then? Because for the price and because it is a product without most of the bad stuff, this is a good deal. All organic shampoos are crazy expensive. So products that have mostly good stuff for an affordable price is better than going with full chemical stuff. So a good step in the right direction, but not quite there.Finally, as to the product itself: it worked up a lather well (thanks cancer promoting sulfates!) and cleaned out well without residue. It smells very strongly fragrant in the shower but loses the strong scent after a few minutes. The scent doesn't smell like rain or forrest, but does have what we would consider a soapy or clean smell.\n",
      "classified as : 3.0\n",
      "true label was : 2\n",
      "review : I have mix feelings with this mascara. This mascara is ok but not the best. I do like that it doesn't flake, clump and lashes are soft to touch. Other than this thats all the positive I have to say for this mascara. This mascara has cons in my opinion.1. Not sure why its has volum or plush words in the name. There is no evidence that it does these things to your lashes.2. The wand is very odd to work with. Needs some major work. It doesn't work for this mascara liquid.3. You can't remove this mascara with a water base makeup remover. I purchased Maybelline's oil base make up remover and it does get the mascara off.I probably will not purchase again. This mascara doesn't do what it advertised.2/17/2012 UPDATEHave used this mascara several weeks. I too am losing lashes. I use to have alot of lashes. Half my lower lashes are gone on one eye. I would suggest when using the oil remover dont rub but hold a few secs and slide it off the eye lashes. That seems to be helping to keep the few lashes I have. There is a bad chemical smell that waters my eyes and maybe causing my eye lashes coming out in clumps. Revised and gave it one star.\n",
      "classified as : 2.0\n",
      "true label was : 1\n",
      "review : I've been using this body wash for almost a week and am very pleased with the result. It's the dead of winter here, and this season is brutal in my part of the country, not just cold but the air is very dry. Chapped skin is almost a given, moisturizers and lotions a necessity.One of the reasons this product appealed to me was because it is a moisturizing body wash. So, while exfoliation is also desired, the standard net poof I use to apply all body washes accomplishes that goal. My hope was that this product, while cleansing, would not leave my skin feeling dry, and it didn't. The feeling left on my skin from the Dove is soft and smooth. I still use body lotion but my skin seems to be slightly more moisturized before applying my usual cream. I am attributing this change to the body wash.I do not have extremely sensitive skin, but whenever possible, like to avoid unnecessary ingredients which is another reason why I wanted to try this product. This brings me to my only complaint - it isn't unscented.I choose unscented products because layering perfumes from deodorants, soaps, and various creams, one on top of another, is offensive to me and I assume to a large portion of the people around me. If I wish to wear a fragrance, I will, but I would prefer all basic toiletry items remain unscented. So, the fact that this body wash has a definite scent when it claims to be unscented is unfortunate and disappointing. With that one caveat, I liked the product itself, just not its smell.\n",
      "classified as : 2.0\n",
      "true label was : 3\n",
      "review : While many so-called moisturizing cr&egrave;mes (at a much higher price) promise to plump skin and diminish wrinkles, Hyaluraonic Acid Day Cr&egrave;me does just that. This is the solution that is put on the skin before facials in a salon, usually followed by some form of penetration enhancement. From the moment this cr&egrave;me goes on the skin feels `different' - fresh, non-saggy, plumped up. The cr&egrave;me lasts all day long without the need for re-application.The other in gredients in this cr&egrave;me are Water (Aqua), Organic Aloe Barbadensis Leaf Extract, Organic Camellia Sinensis (Green Tea) Leaf Extract, Glycerin, Hamamelis Virginiana (Witch Hazel) Extract, Caprylic/Capric Triglyceride, Stearic Acid, Cetyl Alcohol, Glyceryl Stearate, Ceteareth 20, Organic Simmondsia Chinensis (Jojoba) Seed Oil, Sodium Hyaluronate (Hyaluronic Acid; Actimoist Bio-2), Calcium Ascorbate (Ester-C), Tocopheryl Acetate (Vitamin E), Retinyl Palmitate (Vitamin A), Polysorbate, Dimethicone, Phenoxyethanol, Ethylhexyl. The company is pretty straight forward in telling you what exactly you're getting.  And with daily uses it works to freshen aging skin. Very fine product. Grady Harp, July 13\n",
      "classified as : 2.0\n",
      "true label was : 3\n",
      "review : I really enjoyed the OLD FORMULA  well protein free crece pelo rinse and treatment.  Super thick white cream that requires alot of water to rinse out of the hair.  It worked on both wet or dry hair as it softened yet strengthened my hair. The new formula gets cakey on dry hair and doesn't have the umph so to speak that the old formula had. My hair felt better with drugstore brands than the new formula so I will no longer be using this unless I can luck up and find the old formula on the ground.  I do not need to buy both a separate moisturizing and strengthen treatment for my hair even when using thermal heat appliances when I used the old formula b/c the plant proteins strengthen my hair without the use of synthetic hydrolsyed proteins which dry out and matt up my hair.I am Dominican and have waist length wavy/ thick (4a) long hair that needs moisture, but doesn't do well with hydrolsed proteins. Even henna does my hair a disservice by drying it out. So this is great for maintaining strength without making my hair feel dried out and stiff. But the ingredients changed. It is not as natural as it was in the past for those who are more anal when it comes to ingredients. It also, now has cones (silicone). I have no issue with the particular cone it contains although it's not water soluble and I do not use shampoo on my hair. But, just keep in mind when ordering online you do not know which version you will get. The old formula and the new one has the same packaging. And they don't have a woman on the bottle.\n",
      "classified as : 1.0\n",
      "true label was : 2\n",
      "review : I'm asian and my hair is medium in thickness.  My hair uncurls once I get to the other side of my head.  I've used other ones that were better and lasted me more than one day without any sprays.  This one might work for you if you have fine hair.\n",
      "classified as : 2.0\n",
      "true label was : 1\n",
      "review : This cream is nice and light, soak into skin immediately, but the smell is so strong. I could not tolerate it. I tried to use it as a night cream and literally could not breath. So sorry\n",
      "classified as : 1.0\n",
      "true label was : 2\n",
      "review : Granted, I'm not very gifted when it  comes to styling my hair, but I've had a great deal of success with my T3.  Nonetheless, always eager to try something new I was happy to receive this.Wow!  it really heats quickly, and it is HOT!  My hair is short and I have a slightly  singed ear to prove that it can be dangerous to use if you/re  not very careful.  The heat protective glove is really a necessity.In addition to short hair my hair is also very fine so I had a difficult time keeping it on the wand.  I loaned it to a friend with long, thick hair (who is much  more accomplished at styling than I am) and she was quite pleased.  I've not had much luck with it,  but will keep trying.  My lack of enthusiasm is probably  due to the length of my hair, the fact that it's fine, and my being styling challenged.\n",
      "classified as : 3.0\n",
      "true label was : 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def printMisclassification(i):\n",
    "    print(\"review : \" + X_test_[i])\n",
    "    print(\"classified as : \" +str(ypred[i]))\n",
    "    print (\"true label was : \" + str(y_test[i]))\n",
    "\n",
    "    \n",
    "sampled = np.random.choice(missclassifications, 10)\n",
    "for c in sampled:\n",
    "    printMisclassification(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# discussing 10 reviews <br>\n",
    "review 1 <br>\n",
    "Pros:- lasts long- easy to apply- fast drying- extremely thin brush for precision- non-irritating (I have sensitive skin and am allergic to a lot of makeup)- cheaper than expensive name brand makeup companies in the malls, but quality is just as greatCons:- smudges after long wear <br>\n",
    "classified as : 2.0<br>\n",
    "true label was : 3<br>\n",
    "<br>\n",
    "This review contains a lot of words associated with negative reviews such as allergic but sill is positive. <br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review 2 <br>\n",
    "I have always been a fan of Britney, but never really been a fan of perfume. Scents that are too strong tend to give me headaches. But this one is amazing. It's light enough that you can spray it on a few times without the scent becoming overwhelming, but strong enough so that you can smell it on yourself all day. I also have Fantasy and it has the same light qualities. Both are great scents, although I prefer Curious. I think the bottle is tacky, but that's okay. Love it and will continue to buy it when I run out.<br>\n",
    "classified as : 2.0<br>\n",
    "true label was : 3<br>\n",
    "<br>\n",
    "does contain many good words but \"headaches\" may be associated with a negative. <br>\n",
    "<br>\n",
    "review 3 <br>\n",
    "I'm not sure this thing works but I'm sure it is a bit painful. I'm also not surprised that Angelina Jolie is one of the folks who endorses it....she looks like she's might be into a little pain from time to time, I'm not. If you don't mind feeling like you've gotten a wind burn, you might like to use this small instrument of torture and maybe it will exfoliate or something but I've found no evidence of anything beneficial in my case. <br>\n",
    "classified as : 2.0 <br>\n",
    "true label was : 1 <br>\n",
    "<br>\n",
    "I don't think my model knows who Angelina Jolie is. <br>\n",
    "<br>\n",
    "review 4 <br>\n",
    "I first purchased this hand treatment from a garden center - what a favorite it has become. I really does help restore your hands after digging in the garden (or washing dishes in the sink, if you're old-fashioned like me). I have purchased many as gifts for others. This size (100g) is a generous tube and will last a long time. I gave it four stars only because it is more expensive here than at the Crabtree & Evelyn outlet store - or maybe I just got lucky when I visited that store last time. Truth is, I'll buy it wherever I can find it.<br>\n",
    "classified as : 2.0<br>\n",
    "true label was : 3<br>\n",
    "<br>\n",
    "Not sure, it does mention he/she will give 4 stars which should be a good feature. <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "review 5  <br> \n",
    "This product lives up to it's reviews. It doesn't leave hair with the ick factor, de frizzes curls.<br>\n",
    "classified as : 1.0<br>\n",
    "true label was : 3<br>\n",
    "<br>\n",
    "very little information in this review <br>\n",
    "<br>\n",
    "\n",
    "review  6 <br>\n",
    "Smell is so subjective it seems that it may not be a valid review point. One person may love the smell of something and another person hate it. That being said, I like the smell of the citrus body wash. It has a faint scent of oranges and is sweet. It doesn't dry out my skin as much as other soaps do. It makes mounds of lather, so a little goes a long way. The ingredients listed on the product page are incorrect; it does not contain lauryl or laureth sulfates.<br>\n",
    "classified as : 1.0<br>\n",
    "true label was : 3<br>\n",
    "<br>\n",
    "conatains many negative words <br>\n",
    "<br>\n",
    "\n",
    "review 7 <br>\n",
    "The polish isn't what I thought it would be. I'm not a fan of the color on my skin tone (dark skin). Gelish product is amazing nothing against the product. recieved in a reasonable time.<br>\n",
    "classified as : 1.0<br>\n",
    "true label was : 2<br>\n",
    "<br>\n",
    "Strange review, seems like she didn't like it but still an average review. <br>\n",
    "<br>\n",
    "\n",
    "review 8<br>\n",
    "It works well most of the time, but there are occasions when the heating like will not turn off, and the ready light will not turn on...and it is most definately still heating the wax...but I turn it off for about an hour then turn it back on and that usually does the trick. Even when it does work properly I think it makes the wax a little too hot.Other than that not to bad<br>\n",
    "classified as : 3.0<br>\n",
    "true label was : 2<br>\n",
    "<br>\n",
    "not sure, could be just inconsistency in the dataset. <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "review 9<br>\n",
    "I was tremendously excited to try this after having seen several people with amazing Splat-dyed hair at ComiCon.Unfortunately, my hair apparently wants no part of bleach or purple dye!  :(I did a bunch of research before trying this, and followed all the instructions carefully. I was very happy with the consistency of the bleach and the dye, as well as the packaging. The kit was easy to use, cleaner than some I've tried, and the times suggested for bleaching & dying were completely reasonable.One round of bleach was not enough to sufficiently or evenly strip my (naturally medium brown) hair, but further bleaching resulting in looking like a derelict barbie doll! Despite not naturally having any red in it, my hair bleached down to a reddish blonde and wouldn't budge any farther. The purple looks amazing in the few spots that it actually took, but large parts of my hair barely took any purple dye at all, despite having been bleached and my careful following of the instructions.Frankly, it looked pretty bad, but having spent almost 10 hours on the process from start to finish (large chunks of that time being the required blow-drying to complete dryness between every step) there wasn't much else I could do.The dye does wash out quickly whenever it comes in contact with water at all, but that has actually worked in my favor as it is evening out, and becoming more of a berry shade with blends less obnoxiously with the parts of my hair that are still mysteriously reddish blonde.Ergo, my tips for others are:1. If you have not used this before, consider getting a professional or thoroughly experienced friend to help you in case you too end up needing to do some trouble shooting mid-way through the process.2. If your hair is not already blonde, seriously consider sticking to the darker colors (blue/red) that will still show without bleaching, or buy an extra lightening bleach kit.3. Have a backup plan, and give yourself lots of time to both do this and to potentially fix it before any important dates/events, just in case.Wanted to love this, but won't be using it again.<br>\n",
    "classified as : 2.0<br>\n",
    "true label was : 1<br>\n",
    "<br>\n",
    "Contains both positive and negative features which often indicated an average review. <br>\n",
    "<br>\n",
    "\n",
    "\n",
    "review 10<br>\n",
    "I really believe on Neutrogena sunblock products, but this one doesn't give me the felling that's a 110 SPF. It feels more like 40-50 and doesn't avoid my skin to become red after some exposure to sun.Nevertheless, I prefer to use it on my face instead of a regular/body sunscreen. <br>\n",
    "classified as : 3.0<br>\n",
    "true label was : 2<br>\n",
    "<br>\n",
    "Not sure to be honest<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's have some fun and try a neural network instead <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DataLoader(Dataset):\n",
    "    def __init__(self,x,y,vector,featSelect  ,batch_size= 10 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.reviews = x\n",
    "        self.score = y\n",
    "        self.vect = vector\n",
    "        self.feat_selecter = featSelect \n",
    "    def __getitem__(self,indexes ):\n",
    "        indexes=indexes*batch_size\n",
    "        reviews = self.reviews[indexes: indexes+self.batch_size]\n",
    "        scores = self.score[indexes: indexes + self.batch_size]\n",
    "        features=[]\n",
    "        score_result = np.zeros([len(scores)])\n",
    "        for i, s in enumerate(scores):\n",
    "            score_result[i] = s-1\n",
    "        features = self.vect.transform(reviews).toarray()\n",
    "        features = self.feat_selecter.transform(features)\n",
    "        return features, score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifier, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(11000 , 5500),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(5500, 2750),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2750, 1375),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1375, 275),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(275, 55),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(55, numberOfLabels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x[:,0:11000]\n",
    "        x = torch.Tensor(x)\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 1\n",
    "n_epochs = 5\n",
    "batch_size = 1000\n",
    "vect = vectorizer(1)\n",
    "vect = vect.fit(X_train)\n",
    "feats = vect.transform(X_train)\n",
    "featureSelector = SelectKBest(chi2, k=11000).fit(feats,y_train)\n",
    "\n",
    "\n",
    "train_dataLoader = DataLoader(X_train,y_train,vect,featureSelector,batch_size)\n",
    "test_dataLoader = DataLoader(X_test,y_test,vect,featureSelector,batch_size)\n",
    "\n",
    "\n",
    "network = classifier()\n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    batch_idx = 1\n",
    "    confusion_matrix = np.zeros([5,5])\n",
    "    with torch.no_grad():\n",
    "        for data, target_original in test_dataLoader:\n",
    "            output = network(data)\n",
    "            target = Variable(torch.from_numpy(target_original)).long()\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            for t, p in zip(target.view(-1), pred.view(-1)):\n",
    "                confusion_matrix[t, p] = int(1 + confusion_matrix[t, p])\n",
    "            batch_idx += 1\n",
    "            if batch_idx * len(data) > len(test_dataLoader.reviews):\n",
    "                break\n",
    "    test_loss= np.mean(test_loss)# /= len(test_dataLoader.reviews)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_dataLoader.reviews),\n",
    "    100. * correct / len(test_dataLoader.reviews)))\n",
    "    print (confusion_matrix)\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    batch_idx = 1\n",
    "    for  data, target in train_dataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        target  =  Variable(torch.from_numpy(target)).long()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "             epoch, batch_idx * len(data), len(train_dataLoader.reviews),\n",
    "             100. * batch_idx *len(data)/ len(train_dataLoader.reviews), loss.item()))\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "            (batch_idx*batch_size) + ((epoch-1)*len(train_dataLoader.reviews)))\n",
    "        batch_idx += 1\n",
    "        if batch_idx * len(data)>= len(train_dataLoader.reviews):\n",
    "            break\n",
    "    torch.save(network.state_dict(), 'model.pth')\n",
    "    torch.save(optimizer.state_dict(), 'optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 13.1995, Accuracy: 4072/12000 (33%)\n",
      "\n",
      "[[4072.    0.    0.    0.    0.]\n",
      " [3999.    0.    0.    0.    0.]\n",
      " [3929.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n",
      "Train Epoch: 1 [1000/48000 (2%)]\tLoss: 1.100209\n",
      "Train Epoch: 1 [2000/48000 (4%)]\tLoss: 1.464941\n",
      "Train Epoch: 1 [3000/48000 (6%)]\tLoss: 1.100806\n",
      "Train Epoch: 1 [4000/48000 (8%)]\tLoss: 1.100465\n",
      "Train Epoch: 1 [5000/48000 (10%)]\tLoss: 1.102158\n",
      "Train Epoch: 1 [6000/48000 (12%)]\tLoss: 1.109353\n",
      "Train Epoch: 1 [7000/48000 (15%)]\tLoss: 1.102823\n",
      "Train Epoch: 1 [8000/48000 (17%)]\tLoss: 1.105120\n",
      "Train Epoch: 1 [9000/48000 (19%)]\tLoss: 1.088193\n",
      "Train Epoch: 1 [10000/48000 (21%)]\tLoss: 1.086969\n",
      "Train Epoch: 1 [11000/48000 (23%)]\tLoss: 1.045844\n",
      "Train Epoch: 1 [12000/48000 (25%)]\tLoss: 1.000295\n",
      "Train Epoch: 1 [13000/48000 (27%)]\tLoss: 0.997312\n",
      "Train Epoch: 1 [14000/48000 (29%)]\tLoss: 1.003915\n",
      "Train Epoch: 1 [15000/48000 (31%)]\tLoss: 1.045543\n",
      "Train Epoch: 1 [16000/48000 (33%)]\tLoss: 0.885093\n",
      "Train Epoch: 1 [17000/48000 (35%)]\tLoss: 0.893071\n",
      "Train Epoch: 1 [18000/48000 (38%)]\tLoss: 0.875975\n",
      "Train Epoch: 1 [19000/48000 (40%)]\tLoss: 0.810140\n",
      "Train Epoch: 1 [20000/48000 (42%)]\tLoss: 0.887517\n",
      "Train Epoch: 1 [21000/48000 (44%)]\tLoss: 0.842506\n",
      "Train Epoch: 1 [22000/48000 (46%)]\tLoss: 0.844549\n",
      "Train Epoch: 1 [23000/48000 (48%)]\tLoss: 0.828187\n",
      "Train Epoch: 1 [24000/48000 (50%)]\tLoss: 0.832616\n",
      "Train Epoch: 1 [25000/48000 (52%)]\tLoss: 0.816960\n",
      "Train Epoch: 1 [26000/48000 (54%)]\tLoss: 0.851404\n",
      "Train Epoch: 1 [27000/48000 (56%)]\tLoss: 0.827124\n",
      "Train Epoch: 1 [28000/48000 (58%)]\tLoss: 0.834601\n",
      "Train Epoch: 1 [29000/48000 (60%)]\tLoss: 0.818143\n",
      "Train Epoch: 1 [30000/48000 (62%)]\tLoss: 0.830294\n",
      "Train Epoch: 1 [31000/48000 (65%)]\tLoss: 0.809082\n",
      "Train Epoch: 1 [32000/48000 (67%)]\tLoss: 0.820490\n",
      "Train Epoch: 1 [33000/48000 (69%)]\tLoss: 0.845950\n",
      "Train Epoch: 1 [34000/48000 (71%)]\tLoss: 0.790005\n",
      "Train Epoch: 1 [35000/48000 (73%)]\tLoss: 0.829722\n",
      "Train Epoch: 1 [36000/48000 (75%)]\tLoss: 0.772644\n",
      "Train Epoch: 1 [37000/48000 (77%)]\tLoss: 0.782894\n",
      "Train Epoch: 1 [38000/48000 (79%)]\tLoss: 0.785183\n",
      "Train Epoch: 1 [39000/48000 (81%)]\tLoss: 0.776671\n",
      "Train Epoch: 1 [40000/48000 (83%)]\tLoss: 0.758128\n",
      "Train Epoch: 1 [41000/48000 (85%)]\tLoss: 0.789497\n",
      "Train Epoch: 1 [42000/48000 (88%)]\tLoss: 0.734109\n",
      "Train Epoch: 1 [43000/48000 (90%)]\tLoss: 0.758961\n",
      "Train Epoch: 1 [44000/48000 (92%)]\tLoss: 0.753383\n",
      "Train Epoch: 1 [45000/48000 (94%)]\tLoss: 0.745049\n",
      "Train Epoch: 1 [46000/48000 (96%)]\tLoss: 0.767246\n",
      "Train Epoch: 1 [47000/48000 (98%)]\tLoss: 0.719823\n",
      "\n",
      "Test set: Avg. loss: 8.9971, Accuracy: 7920/12000 (66%)\n",
      "\n",
      "[[2325. 1401.  346.    0.    0.]\n",
      " [ 717. 2480.  802.    0.    0.]\n",
      " [ 148.  666. 3115.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n",
      "Train Epoch: 2 [1000/48000 (2%)]\tLoss: 0.719059\n",
      "Train Epoch: 2 [2000/48000 (4%)]\tLoss: 0.727622\n",
      "Train Epoch: 2 [3000/48000 (6%)]\tLoss: 0.681830\n",
      "Train Epoch: 2 [4000/48000 (8%)]\tLoss: 0.638366\n",
      "Train Epoch: 2 [5000/48000 (10%)]\tLoss: 0.711018\n",
      "Train Epoch: 2 [6000/48000 (12%)]\tLoss: 0.615884\n",
      "Train Epoch: 2 [7000/48000 (15%)]\tLoss: 0.748205\n",
      "Train Epoch: 2 [8000/48000 (17%)]\tLoss: 0.688525\n",
      "Train Epoch: 2 [9000/48000 (19%)]\tLoss: 0.769864\n",
      "Train Epoch: 2 [10000/48000 (21%)]\tLoss: 0.676565\n",
      "Train Epoch: 2 [11000/48000 (23%)]\tLoss: 0.696936\n",
      "Train Epoch: 2 [12000/48000 (25%)]\tLoss: 0.667215\n",
      "Train Epoch: 2 [13000/48000 (27%)]\tLoss: 0.722599\n",
      "Train Epoch: 2 [14000/48000 (29%)]\tLoss: 0.655195\n",
      "Train Epoch: 2 [15000/48000 (31%)]\tLoss: 0.640311\n",
      "Train Epoch: 2 [16000/48000 (33%)]\tLoss: 0.656622\n",
      "Train Epoch: 2 [17000/48000 (35%)]\tLoss: 0.632154\n",
      "Train Epoch: 2 [18000/48000 (38%)]\tLoss: 0.655347\n",
      "Train Epoch: 2 [19000/48000 (40%)]\tLoss: 0.610931\n",
      "Train Epoch: 2 [20000/48000 (42%)]\tLoss: 0.665780\n",
      "Train Epoch: 2 [21000/48000 (44%)]\tLoss: 0.649289\n",
      "Train Epoch: 2 [22000/48000 (46%)]\tLoss: 0.649441\n",
      "Train Epoch: 2 [23000/48000 (48%)]\tLoss: 0.641262\n",
      "Train Epoch: 2 [24000/48000 (50%)]\tLoss: 0.610383\n",
      "Train Epoch: 2 [25000/48000 (52%)]\tLoss: 0.606916\n",
      "Train Epoch: 2 [26000/48000 (54%)]\tLoss: 0.610801\n",
      "Train Epoch: 2 [27000/48000 (56%)]\tLoss: 0.624883\n",
      "Train Epoch: 2 [28000/48000 (58%)]\tLoss: 0.612360\n",
      "Train Epoch: 2 [29000/48000 (60%)]\tLoss: 0.598421\n",
      "Train Epoch: 2 [30000/48000 (62%)]\tLoss: 0.626617\n",
      "Train Epoch: 2 [31000/48000 (65%)]\tLoss: 0.597022\n",
      "Train Epoch: 2 [32000/48000 (67%)]\tLoss: 0.627524\n",
      "Train Epoch: 2 [33000/48000 (69%)]\tLoss: 0.622760\n",
      "Train Epoch: 2 [34000/48000 (71%)]\tLoss: 0.593758\n",
      "Train Epoch: 2 [35000/48000 (73%)]\tLoss: 0.619475\n",
      "Train Epoch: 2 [36000/48000 (75%)]\tLoss: 0.569248\n",
      "Train Epoch: 2 [37000/48000 (77%)]\tLoss: 0.590974\n",
      "Train Epoch: 2 [38000/48000 (79%)]\tLoss: 0.556063\n",
      "Train Epoch: 2 [39000/48000 (81%)]\tLoss: 0.575633\n",
      "Train Epoch: 2 [40000/48000 (83%)]\tLoss: 0.543462\n",
      "Train Epoch: 2 [41000/48000 (85%)]\tLoss: 0.590734\n",
      "Train Epoch: 2 [42000/48000 (88%)]\tLoss: 0.554547\n",
      "Train Epoch: 2 [43000/48000 (90%)]\tLoss: 0.546252\n",
      "Train Epoch: 2 [44000/48000 (92%)]\tLoss: 0.570402\n",
      "Train Epoch: 2 [45000/48000 (94%)]\tLoss: 0.551764\n",
      "Train Epoch: 2 [46000/48000 (96%)]\tLoss: 0.595168\n",
      "Train Epoch: 2 [47000/48000 (98%)]\tLoss: 0.552890\n",
      "\n",
      "Test set: Avg. loss: 9.2538, Accuracy: 8030/12000 (66%)\n",
      "\n",
      "[[2612. 1220.  240.    0.    0.]\n",
      " [ 872. 2716.  411.    0.    0.]\n",
      " [ 338.  889. 2702.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n",
      "Train Epoch: 3 [1000/48000 (2%)]\tLoss: 0.545654\n",
      "Train Epoch: 3 [2000/48000 (4%)]\tLoss: 0.585320\n",
      "Train Epoch: 3 [3000/48000 (6%)]\tLoss: 0.508877\n",
      "Train Epoch: 3 [4000/48000 (8%)]\tLoss: 0.506635\n",
      "Train Epoch: 3 [5000/48000 (10%)]\tLoss: 0.504580\n",
      "Train Epoch: 3 [6000/48000 (12%)]\tLoss: 0.462807\n",
      "Train Epoch: 3 [7000/48000 (15%)]\tLoss: 0.479186\n",
      "Train Epoch: 3 [8000/48000 (17%)]\tLoss: 0.473269\n",
      "Train Epoch: 3 [9000/48000 (19%)]\tLoss: 0.593710\n",
      "Train Epoch: 3 [10000/48000 (21%)]\tLoss: 0.548519\n",
      "Train Epoch: 3 [11000/48000 (23%)]\tLoss: 0.544675\n",
      "Train Epoch: 3 [12000/48000 (25%)]\tLoss: 0.535555\n",
      "Train Epoch: 3 [13000/48000 (27%)]\tLoss: 0.565654\n",
      "Train Epoch: 3 [14000/48000 (29%)]\tLoss: 0.498609\n",
      "Train Epoch: 3 [15000/48000 (31%)]\tLoss: 0.507058\n",
      "Train Epoch: 3 [16000/48000 (33%)]\tLoss: 0.490756\n",
      "Train Epoch: 3 [17000/48000 (35%)]\tLoss: 0.467246\n",
      "Train Epoch: 3 [18000/48000 (38%)]\tLoss: 0.518429\n",
      "Train Epoch: 3 [19000/48000 (40%)]\tLoss: 0.497803\n",
      "Train Epoch: 3 [20000/48000 (42%)]\tLoss: 0.512295\n",
      "Train Epoch: 3 [21000/48000 (44%)]\tLoss: 0.514100\n",
      "Train Epoch: 3 [22000/48000 (46%)]\tLoss: 0.527138\n",
      "Train Epoch: 3 [23000/48000 (48%)]\tLoss: 0.509524\n",
      "Train Epoch: 3 [24000/48000 (50%)]\tLoss: 0.468939\n",
      "Train Epoch: 3 [25000/48000 (52%)]\tLoss: 0.518866\n",
      "Train Epoch: 3 [26000/48000 (54%)]\tLoss: 0.498707\n",
      "Train Epoch: 3 [27000/48000 (56%)]\tLoss: 0.509762\n",
      "Train Epoch: 3 [28000/48000 (58%)]\tLoss: 0.519318\n",
      "Train Epoch: 3 [29000/48000 (60%)]\tLoss: 0.467420\n",
      "Train Epoch: 3 [30000/48000 (62%)]\tLoss: 0.478777\n",
      "Train Epoch: 3 [31000/48000 (65%)]\tLoss: 0.502337\n",
      "Train Epoch: 3 [32000/48000 (67%)]\tLoss: 0.491813\n",
      "Train Epoch: 3 [33000/48000 (69%)]\tLoss: 0.451500\n",
      "Train Epoch: 3 [34000/48000 (71%)]\tLoss: 0.455401\n",
      "Train Epoch: 3 [35000/48000 (73%)]\tLoss: 0.477467\n",
      "Train Epoch: 3 [36000/48000 (75%)]\tLoss: 0.442610\n",
      "Train Epoch: 3 [37000/48000 (77%)]\tLoss: 0.472289\n",
      "Train Epoch: 3 [38000/48000 (79%)]\tLoss: 0.455266\n",
      "Train Epoch: 3 [39000/48000 (81%)]\tLoss: 0.418110\n",
      "Train Epoch: 3 [40000/48000 (83%)]\tLoss: 0.397372\n",
      "Train Epoch: 3 [41000/48000 (85%)]\tLoss: 0.432295\n",
      "Train Epoch: 3 [42000/48000 (88%)]\tLoss: 0.427357\n",
      "Train Epoch: 3 [43000/48000 (90%)]\tLoss: 0.416951\n",
      "Train Epoch: 3 [44000/48000 (92%)]\tLoss: 0.432092\n",
      "Train Epoch: 3 [45000/48000 (94%)]\tLoss: 0.475392\n",
      "Train Epoch: 3 [46000/48000 (96%)]\tLoss: 0.464858\n",
      "Train Epoch: 3 [47000/48000 (98%)]\tLoss: 0.422914\n",
      "\n",
      "Test set: Avg. loss: 12.3742, Accuracy: 7843/12000 (65%)\n",
      "\n",
      "[[2355. 1337.  380.    0.    0.]\n",
      " [ 737. 2531.  731.    0.    0.]\n",
      " [ 276.  696. 2957.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n",
      "Train Epoch: 4 [1000/48000 (2%)]\tLoss: 0.417228\n",
      "Train Epoch: 4 [2000/48000 (4%)]\tLoss: 0.408228\n",
      "Train Epoch: 4 [3000/48000 (6%)]\tLoss: 0.433790\n",
      "Train Epoch: 4 [4000/48000 (8%)]\tLoss: 0.423798\n",
      "Train Epoch: 4 [5000/48000 (10%)]\tLoss: 0.425028\n",
      "Train Epoch: 4 [6000/48000 (12%)]\tLoss: 0.336689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [7000/48000 (15%)]\tLoss: 0.476251\n",
      "Train Epoch: 4 [8000/48000 (17%)]\tLoss: 0.335642\n",
      "Train Epoch: 4 [9000/48000 (19%)]\tLoss: 0.411949\n",
      "Train Epoch: 4 [10000/48000 (21%)]\tLoss: 0.441961\n",
      "Train Epoch: 4 [11000/48000 (23%)]\tLoss: 0.480178\n",
      "Train Epoch: 4 [12000/48000 (25%)]\tLoss: 0.491087\n",
      "Train Epoch: 4 [13000/48000 (27%)]\tLoss: 0.452302\n",
      "Train Epoch: 4 [14000/48000 (29%)]\tLoss: 0.415548\n",
      "Train Epoch: 4 [15000/48000 (31%)]\tLoss: 0.429972\n",
      "Train Epoch: 4 [16000/48000 (33%)]\tLoss: 0.432572\n",
      "Train Epoch: 4 [17000/48000 (35%)]\tLoss: 0.358740\n",
      "Train Epoch: 4 [18000/48000 (38%)]\tLoss: 0.365505\n",
      "Train Epoch: 4 [19000/48000 (40%)]\tLoss: 0.370196\n",
      "Train Epoch: 4 [20000/48000 (42%)]\tLoss: 0.400161\n",
      "Train Epoch: 4 [21000/48000 (44%)]\tLoss: 0.410911\n",
      "Train Epoch: 4 [22000/48000 (46%)]\tLoss: 0.423808\n",
      "Train Epoch: 4 [23000/48000 (48%)]\tLoss: 0.364050\n",
      "Train Epoch: 4 [24000/48000 (50%)]\tLoss: 0.371397\n",
      "Train Epoch: 4 [25000/48000 (52%)]\tLoss: 0.355768\n",
      "Train Epoch: 4 [26000/48000 (54%)]\tLoss: 0.376083\n",
      "Train Epoch: 4 [27000/48000 (56%)]\tLoss: 0.352675\n",
      "Train Epoch: 4 [28000/48000 (58%)]\tLoss: 0.351203\n",
      "Train Epoch: 4 [29000/48000 (60%)]\tLoss: 0.356797\n",
      "Train Epoch: 4 [30000/48000 (62%)]\tLoss: 0.318978\n",
      "Train Epoch: 4 [31000/48000 (65%)]\tLoss: 0.373002\n",
      "Train Epoch: 4 [32000/48000 (67%)]\tLoss: 0.347670\n",
      "Train Epoch: 4 [33000/48000 (69%)]\tLoss: 0.339002\n",
      "Train Epoch: 4 [34000/48000 (71%)]\tLoss: 0.295712\n",
      "Train Epoch: 4 [35000/48000 (73%)]\tLoss: 0.303192\n",
      "Train Epoch: 4 [36000/48000 (75%)]\tLoss: 0.371210\n",
      "Train Epoch: 4 [37000/48000 (77%)]\tLoss: 0.433973\n",
      "Train Epoch: 4 [38000/48000 (79%)]\tLoss: 0.352567\n",
      "Train Epoch: 4 [39000/48000 (81%)]\tLoss: 0.427712\n",
      "Train Epoch: 4 [40000/48000 (83%)]\tLoss: 0.348190\n",
      "Train Epoch: 4 [41000/48000 (85%)]\tLoss: 0.344864\n",
      "Train Epoch: 4 [42000/48000 (88%)]\tLoss: 0.314511\n",
      "Train Epoch: 4 [43000/48000 (90%)]\tLoss: 0.374808\n",
      "Train Epoch: 4 [44000/48000 (92%)]\tLoss: 0.370818\n",
      "Train Epoch: 4 [45000/48000 (94%)]\tLoss: 0.349857\n",
      "Train Epoch: 4 [46000/48000 (96%)]\tLoss: 0.341279\n",
      "Train Epoch: 4 [47000/48000 (98%)]\tLoss: 0.371484\n",
      "\n",
      "Test set: Avg. loss: 14.9296, Accuracy: 7744/12000 (64%)\n",
      "\n",
      "[[2163. 1425.  484.    0.    0.]\n",
      " [ 687. 2508.  804.    0.    0.]\n",
      " [ 153.  703. 3073.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n",
      "Train Epoch: 5 [1000/48000 (2%)]\tLoss: 0.332318\n",
      "Train Epoch: 5 [2000/48000 (4%)]\tLoss: 0.335646\n",
      "Train Epoch: 5 [3000/48000 (6%)]\tLoss: 0.274009\n",
      "Train Epoch: 5 [4000/48000 (8%)]\tLoss: 0.282430\n",
      "Train Epoch: 5 [5000/48000 (10%)]\tLoss: 0.378052\n",
      "Train Epoch: 5 [6000/48000 (12%)]\tLoss: 0.315475\n",
      "Train Epoch: 5 [7000/48000 (15%)]\tLoss: 0.268815\n",
      "Train Epoch: 5 [8000/48000 (17%)]\tLoss: 0.227659\n",
      "Train Epoch: 5 [9000/48000 (19%)]\tLoss: 0.394620\n",
      "Train Epoch: 5 [10000/48000 (21%)]\tLoss: 0.286741\n",
      "Train Epoch: 5 [11000/48000 (23%)]\tLoss: 0.217307\n",
      "Train Epoch: 5 [12000/48000 (25%)]\tLoss: 0.272091\n",
      "Train Epoch: 5 [13000/48000 (27%)]\tLoss: 0.346994\n",
      "Train Epoch: 5 [14000/48000 (29%)]\tLoss: 0.319346\n",
      "Train Epoch: 5 [15000/48000 (31%)]\tLoss: 0.290702\n",
      "Train Epoch: 5 [16000/48000 (33%)]\tLoss: 0.244012\n",
      "Train Epoch: 5 [17000/48000 (35%)]\tLoss: 0.290828\n",
      "Train Epoch: 5 [18000/48000 (38%)]\tLoss: 0.289677\n",
      "Train Epoch: 5 [19000/48000 (40%)]\tLoss: 0.242524\n",
      "Train Epoch: 5 [20000/48000 (42%)]\tLoss: 0.200829\n",
      "Train Epoch: 5 [21000/48000 (44%)]\tLoss: 0.253663\n",
      "Train Epoch: 5 [22000/48000 (46%)]\tLoss: 0.275042\n",
      "Train Epoch: 5 [23000/48000 (48%)]\tLoss: 0.243308\n",
      "Train Epoch: 5 [24000/48000 (50%)]\tLoss: 0.227888\n",
      "Train Epoch: 5 [25000/48000 (52%)]\tLoss: 0.228243\n",
      "Train Epoch: 5 [26000/48000 (54%)]\tLoss: 0.255424\n",
      "Train Epoch: 5 [27000/48000 (56%)]\tLoss: 0.322606\n",
      "Train Epoch: 5 [28000/48000 (58%)]\tLoss: 0.338825\n",
      "Train Epoch: 5 [29000/48000 (60%)]\tLoss: 0.285788\n",
      "Train Epoch: 5 [30000/48000 (62%)]\tLoss: 0.243813\n",
      "Train Epoch: 5 [31000/48000 (65%)]\tLoss: 0.418211\n",
      "Train Epoch: 5 [32000/48000 (67%)]\tLoss: 0.361851\n",
      "Train Epoch: 5 [33000/48000 (69%)]\tLoss: 0.248499\n",
      "Train Epoch: 5 [34000/48000 (71%)]\tLoss: 0.252904\n",
      "Train Epoch: 5 [35000/48000 (73%)]\tLoss: 0.357522\n",
      "Train Epoch: 5 [36000/48000 (75%)]\tLoss: 0.317587\n",
      "Train Epoch: 5 [37000/48000 (77%)]\tLoss: 0.267887\n",
      "Train Epoch: 5 [38000/48000 (79%)]\tLoss: 0.204847\n",
      "Train Epoch: 5 [39000/48000 (81%)]\tLoss: 0.243334\n",
      "Train Epoch: 5 [40000/48000 (83%)]\tLoss: 0.240249\n",
      "Train Epoch: 5 [41000/48000 (85%)]\tLoss: 0.228652\n",
      "Train Epoch: 5 [42000/48000 (88%)]\tLoss: 0.237280\n",
      "Train Epoch: 5 [43000/48000 (90%)]\tLoss: 0.189269\n",
      "Train Epoch: 5 [44000/48000 (92%)]\tLoss: 0.253540\n",
      "Train Epoch: 5 [45000/48000 (94%)]\tLoss: 0.224415\n",
      "Train Epoch: 5 [46000/48000 (96%)]\tLoss: 0.188561\n",
      "Train Epoch: 5 [47000/48000 (98%)]\tLoss: 0.167956\n",
      "\n",
      "Test set: Avg. loss: 16.8025, Accuracy: 7761/12000 (64%)\n",
      "\n",
      "[[2631. 1016.  425.    0.    0.]\n",
      " [1102. 2128.  769.    0.    0.]\n",
      " [ 304.  623. 3002.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcc67f9d2b0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_axis = range(0,len(train_losses))\n",
    "plt.plot(x_axis,train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
